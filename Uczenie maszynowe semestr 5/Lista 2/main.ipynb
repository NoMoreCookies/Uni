{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9a729d",
   "metadata": {},
   "source": [
    "# Konstrukcja sieci neuronowej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0607e0",
   "metadata": {},
   "source": [
    "Rozważmy sieć neuronową składającą się z warstw $1,...,L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65584e94",
   "metadata": {},
   "source": [
    "Gdzie:\n",
    "- $n_l - $ liczba neuronów w warstwie l\n",
    "- $a^l \\in \\mathbb{R}^{n_l}$ - wektor aktywacji warstwy $l$\n",
    "- $z^l \\in \\mathbb{R}^{n_l}$ - wektor sum ważonych przed aktywacją\n",
    "- $W^l \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ - macierz wag między warstwą $l-1$ i $l$\n",
    "- $b^l \\in \\mathbb{R}^{n_l}$ - wektor biasów warstwy $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448c162",
   "metadata": {},
   "source": [
    "## Feedforward (obliczanie wyjścia sieci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e64a8",
   "metadata": {},
   "source": [
    "1.1 **Suma ważona**\n",
    "    $z^l = W^la^{l-1}+b^l$\n",
    "\n",
    "1.2 **Funkcja aktywacji (sigmoid):**\n",
    "    $a^l = \\sigma(z^l)$\n",
    "    $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Wyjściem sieci jest wektor :\n",
    "    $\\hat{y} = a^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7c50e",
   "metadata": {},
   "source": [
    "## Funkcja kosztu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0b6b4",
   "metadata": {},
   "source": [
    "Dla pojedynczej próbki (x,y)  definiujemy koszt kwadratowy:\n",
    "\n",
    "$ C = \\frac{1}{2}{||a^L-y||}^2 $\n",
    "\n",
    "Pochodna po aktywacji w warstwie wyjściowej\n",
    "\n",
    "$ \\frac{\\partial{C}}{\\partial{a^L}} = a^L - y $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e4261",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee38d3",
   "metadata": {},
   "source": [
    "Celem backpropagation jest policzyć \n",
    "$\\frac{\\partial{C}}{\\partial{W^l}},\\frac{\\partial{C}}{\\partial{b^l}}$\n",
    "\n",
    "dla wszystkich warstw $l = 1,2,3...,L$\n",
    "\n",
    "Definiujemy pomocniczą wielkość:\n",
    "\n",
    "${\\delta}^l = \\frac{\\partial{C}}{\\partial{z^l}}$\n",
    "\n",
    "nazywaną błędem warstwy l."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb117f3",
   "metadata": {},
   "source": [
    "## Błąd w warstwie wyjściowej L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4e32d",
   "metadata": {},
   "source": [
    "Wykorzystując wzory:\n",
    "\n",
    "${\\delta}^l = ({\\frac{\\partial{C}}{\\partial{z^l}}})\\odot {\\sigma}'(z^L)$\n",
    "\n",
    "gdzie :\n",
    "\n",
    "$\\sigma'(z) = (a^L-y)\\odot{\\sigma'(z^L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586314bc",
   "metadata": {},
   "source": [
    "## Błąd w warstwach ukrytych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872a834",
   "metadata": {},
   "source": [
    "Dla każdej warstwy $l=L-1, L-2,...,1$:\n",
    "\n",
    "\n",
    "$\\delta^l = (({W^{l+1}})^T{\\delta}^{l+1})\\odot{\\sigma'(z^l)}$\n",
    "\n",
    "Interpretacja :\n",
    "- błędy warstwy następnej wracają wstecz przez transponowaną macierz wag\n",
    "- pochodna sigmoidu przeskalowuje wpływ na aktualną warstwę\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ce7d2",
   "metadata": {},
   "source": [
    "## Gradient wag i biasów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6e1ac",
   "metadata": {},
   "source": [
    "**Gradient biasów**\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{b^l}} = \\delta^l$\n",
    "\n",
    "**Gradient wag**\n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{W^l}} = {\\delta^l}(a^{l-1})^T$\n",
    "\n",
    "Interpretacja:\n",
    "\n",
    "- błąd neuronu $\\theta^l$\n",
    "- pomnożony przez aktywację neurony z  warstwy wcześniejszej\n",
    "- daje wpływ każdej wagi na błąd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753ac66",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e96388",
   "metadata": {},
   "source": [
    "Uczenie polega na aktualizacji wag i biasów:\n",
    "\n",
    "$W^l \\leftarrow W^l - \\eta * \\frac{\\partial{C}}{\\partial{W^l}}$\n",
    "\n",
    "$b^l \\leftarrow b^l - \\eta * \\frac{\\partial{C}}{\\partial{b^l}}$\n",
    "\n",
    "gdzie :\n",
    "\n",
    "- $\\eta$ - learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6effdae",
   "metadata": {},
   "source": [
    "## Praktyka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255bb74",
   "metadata": {},
   "source": [
    "W praktyce liczymy gradienty nie dla pojedynczej próbki, ale dla mibi-batcha\n",
    "\n",
    "Niech mini-batch ma m elementów:\n",
    "\n",
    "$B = {[(x_1,y_1),(x_2,y_2),...,(x_m,y_m)]}$\n",
    "\n",
    "Gradienty sumujemy:\n",
    "\n",
    "$$\n",
    "\\nabla_{W^l} C_B = \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial W^l}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{b^l} C_B = \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial b^l}\n",
    "$$\n",
    "\n",
    "I aktualizacja wygląda tak:\n",
    "\n",
    "$$\n",
    "W^l \\leftarrow W^l - \\frac{\\eta}{m}\\,\\nabla_{W^l} C_B\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^l \\leftarrow b^l - \\frac{\\eta}{m}\\,\\nabla_{b^l} C_B\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fro matrxi calculations\n",
    "import numpy as np\n",
    "\n",
    "# for random test data generation\n",
    "import random\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"Initializes our Network object\n",
    "\n",
    "        Initializes Network object with given sizs of hidden layers\n",
    "\n",
    "        Args:\n",
    "            sizes (list of ints)  - lengths of hiddens layers\n",
    "        \"\"\"\n",
    "        # number of hidden layers\n",
    "        self.num_layers = len(sizes)\n",
    "\n",
    "        # random generated biases\n",
    "        self.biases = [np.random.randn(y, 1)\n",
    "                       for y in sizes[1:]]\n",
    "        \n",
    "        # random generated weights\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "           \n",
    "    def feedforward(self, a):\n",
    "        \"\"\" FeedForward func \n",
    "\n",
    "        It is responsible for calculating actiations for \n",
    "        next neureons in our neural network\n",
    "\n",
    "        Args:\n",
    "            self (Network) : Object\n",
    "            a (float list) : Outputs fromm last layers\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "\n",
    "            # calculating with sigmoid function\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "\n",
    "        return a\n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Stochastic Gradient Descent\n",
    "        \n",
    "        Finds local minima for weights and biases of our neural network \n",
    "        using Stochastic Gradient Descent Algorithm\n",
    "\n",
    "        Args:\n",
    "            self (Network) : Object\n",
    "            training_data (list of pairs) : Image and number on image\n",
    "            epochs (int) :  Number of learning steps of our network\n",
    "            mini_batch_size (int) :  Length of batches that training data should be divied into\n",
    "            eta (float) : Length of our SGD step \n",
    "            test_data (list of pairs) : Image and number on image\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        n = len(training_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "\n",
    "            #random training data shuffle\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            # rendering list of mini batches\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size )\n",
    "                ] \n",
    "            \n",
    "            # teaching our nneural network on with every batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            # evaluating performance of our neural network\n",
    "            # after previous learning step\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\" Updates neural network with single batch\n",
    "\n",
    "            Executes single SGD step on single mini_batch\n",
    "\n",
    "            Args:\n",
    "                mini_batch (list of pairs) : Image and number on image\n",
    "                eta (float) : length of SGD step\n",
    "        \"\"\"\n",
    "\n",
    "        # dla każdej warstwy w każdej sieci są biases\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # dla każdej warstwy w każdej sieci są weights\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            # propagacja wsteczna\n",
    "\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "\n",
    "\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "\n",
    "\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        # update biasów i wag\n",
    "\n",
    "        self.weights = [w - (eta/len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        self.biases = [b - (eta/len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        # tworzymy wetkroy zer dla b \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # tworzymy wetkroy zer dla b \n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # aktywacje\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # przemnazamy aktywacje prze wagi i dodajemy bias\n",
    "            z = np.dot(w,activation)+b\n",
    "            zs.append(z)\n",
    "\n",
    "            # przepuszczamy przez funckje sigmoid\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1] )\n",
    "\n",
    "        nabla_b[-1] = delta\n",
    "\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "\n",
    "            z = zs[-l]\n",
    "\n",
    "            sp = sigmoid_prime(z)\n",
    "\n",
    "            delta = np.dot(self.weights[-l+1].transpose(),delta) * sp\n",
    "\n",
    "            nabla_b[-l] = delta\n",
    "            \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) \n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    # funckja aktywacyjna\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations - y)\n",
    "    \n",
    "# funcjja sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# jakis dziwny sigmoid, nie rozumiem go\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
