{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6471a08",
   "metadata": {},
   "source": [
    "importowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fd5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eb4b1",
   "metadata": {},
   "source": [
    "$ \\frac{1}{2}\\sum_{i=1}^{n}{(y_i-\\hat{f}{(x_i)})}^2 $ \\\n",
    "Gdzie to jest równe \\\n",
    "$ \\frac{1}{2}\\sum_{i=1}^{n}{(y_i-{\\beta}_0-{{\\beta}_1}x_{i1}-{{\\beta}_2}x_{i2}-...-{{\\beta}_n}x_{in}) }^2 $ \\\n",
    "ale u nas \n",
    "$ \\frac{1}{2}\\sum_{i=1}^{n}{(y_i-{{\\beta}_0}-{{\\beta}_1}x_{i1}-{{\\beta}_2}x_{i2}-...-{{\\beta}_m}x_{im}) }^2 + {\\frac{1}{2}}{\\alpha}\\sum_{j=1}^{m}{({{\\beta}_j}^2)} $ \\\n",
    "Cel: Znaleźć ${\\beta}_0,{\\beta}_1 ...itd$ dla których $L(\\beta)$ jest minimalna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b380c",
   "metadata": {},
   "source": [
    "1. Wybieramy jakieś początkowe $\\beta = {\\beta}_0,\\cdots,{\\beta}_n$ \\\n",
    "2. Powtarzamy \\\n",
    "    Liczymy $\\frac{dL}{d{\\beta}_j}$ używając definicji ${beta}_j$ \\\n",
    "    Bierzemy (nowe ${\\beta}_j$ = (stare ${\\beta}_j$) - $c\\frac{dL}{d{\\beta}_j} $)  \n",
    "    gdzie c to pewna stała dodatnia (stała uczenia np: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegr:\n",
    "    def __init__(self, alpha=0.0, tol=1e-12):\n",
    "        self.alpha = float(alpha)\n",
    "        self.tol = float(tol)\n",
    "        self.beta = None \n",
    "\n",
    "    def dL(self, X, Y,j):\n",
    "        \"\"\"Compute the derivate of the loss function with respect to beta_j\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): x values\n",
    "            Y (np.ndarray): y values\n",
    "            j (int): Index of the coefficient\n",
    "\n",
    "        Returns:\n",
    "            float: The derivative of the loss function with respect to beta_j\n",
    "        \"\"\"\n",
    "\n",
    "        n, m = X.shape\n",
    "        suma = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            suma += (Y[i] - X[i,:] @ self.beta) *( -X[i,j])\n",
    "        return suma + (self.alpha * self.beta[j] if j > 0 else 0.0)\n",
    "         \n",
    "\n",
    "    def fit(self, X, Y, c=0.01):\n",
    "        \"\"\"Fits the model to the data using gradient descent\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): x values\n",
    "            Y (np.ndarray): y values\n",
    "            c (float, optional): Learning rate. Defaults to 0.0001.\n",
    "        \n",
    "        Returns:\n",
    "            RidgeRegr: The fitted model\n",
    "        \"\"\"\n",
    "        \n",
    "        n, m = X.shape\n",
    "\n",
    "        X = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "\n",
    "        self.beta = np.zeros(m + 1, dtype=float)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            beta_old = self.beta.copy()\n",
    "\n",
    "            for j in range(m+1):\n",
    "                self.beta[j] -= c * self.dL(X, Y, j)\n",
    "\n",
    "            if np.linalg.norm(self.beta -  beta_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts the output for the given input data\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): x values\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted y values\"\"\"\n",
    "\n",
    "        n, m = X.shape\n",
    "        X = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "\n",
    "        return X @ self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638be177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RidgeRegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,2,10]).reshape((3,1))\n",
    "    alpha = 0.3\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    print(expected)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(actual)\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-5)\n",
    "\n",
    "def test_RidgeRegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    alpha = 0.4\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    print(expected)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(actual)\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4faeed",
   "metadata": {},
   "source": [
    "testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f02c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.88950276  3.38121547 15.31491713]\n",
      "[ 1.88950277  3.38121547 15.31491711]\n",
      "[ 0.54685378 -1.76188321  1.58691716  5.15527388  3.66704391]\n",
      "[ 0.54685371 -1.76188325  1.5869171   5.15527395  3.66704389]\n"
     ]
    }
   ],
   "source": [
    "test_RidgeRegressionInOneDim()\n",
    "test_RidgeRegressionInThreeDim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
