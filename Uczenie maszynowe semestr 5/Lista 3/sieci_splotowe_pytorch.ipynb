{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbab3d2",
   "metadata": {},
   "source": [
    "# Zadanie 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93018762",
   "metadata": {},
   "source": [
    "Zadanie zostało zrobione na podstawie kursu z datacamp : https://campus.datacamp.com/courses/intermediate-deep-learning-with-pytorch/images-convolutional-neural-networks?ex=5 \n",
    "Oraz artykułu cs231 : https://cs231n.github.io/convolutional-networks/?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a738c9",
   "metadata": {},
   "source": [
    "Używam Clouds dataset ( 7 klas ) - z kaggla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de3ec56",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/jockeroika/clouds-photos/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973b729",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a00f8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Precision, Recall\n",
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89075f",
   "metadata": {},
   "source": [
    "Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c130de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting path to dataset\n",
    "path = kagglehub.dataset_download(\"jockeroika/clouds-photos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642069ba",
   "metadata": {},
   "source": [
    "Transformers for input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "17c5b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64)),\n",
    "])\n",
    "\n",
    "test_tranforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03076d21",
   "metadata": {},
   "source": [
    "Preparing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c8dc8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ImageFolder(\n",
    "    path + \"/clouds_train\",\n",
    "    transform=train_transforms,   \n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = 32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_test = ImageFolder(\n",
    "    path +\"/clouds_test\",\n",
    "    transform=test_tranforms\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4eaa7",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c85431",
   "metadata": {},
   "source": [
    "Setting seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6632ac5",
   "metadata": {},
   "source": [
    "Dlaczego ELU a nie RELU - ELU  pomaga w problemie zanikających i wybuchających gradientów oraz  wymierających neuronów\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9496e7",
   "metadata": {},
   "source": [
    "Architektura sieci - artykuł cs231"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67339b",
   "metadata": {},
   "source": [
    "![image](images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fb728",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Budowa sieci podstawowej:\n",
    "    (Conv -> Elu)x2 -> MaxPool ->(Conv -> Elu)x2 -> MaxPool -> Conv->Elu -> MaxPool -> AdaptiveAvg -> DenseLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaddf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 64 x 32 x 32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 64 x 64\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 64 x 64\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 128 x 32 x 32\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 128 x 16 x 16 (jest to bardzo dużo wag - zwiększa ryzyko overfittingu)\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            # adaptive avg pool przeciwdziała przeuczeniu się modelu\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # teraz mamy 128 \n",
    "\n",
    "            \n",
    "        )\n",
    "    \n",
    "        # dense layer at the end\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 64 x 32 x 32\n",
    "\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 64 x 64\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 64 x 64\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            #batchnormalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.ELU(),\n",
    "            # teraz mamy 128 x 32 x 32\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 128 x 16 x 16 (jest to bardzo dużo wag - zwiększa ryzyko overfittingu)\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            # adaptive avg pool przeciwdziała przeuczeniu się modelu\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # teraz mamy 128 \n",
    "\n",
    "            \n",
    "        )\n",
    "    \n",
    "        # dense layer at the end\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2edc03b",
   "metadata": {},
   "source": [
    "Batchnormalization pomaga sieci w uczeniu się (stabilizuje proces uczenia się naszej sieci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 32 x 64 x 64\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 32 x 32 x 32\n",
    "\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 32 x 32\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 64 x 32 x 32\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # teraz mamy 128 x 32 x 32\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # teraz mamy 128 x 16 x 16 (jest to bardzo dużo wag - zwiększa ryzyko overfittingu)\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            # adaptive avg pool przeciwdziała przeuczeniu się modelu\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # teraz mamy 128 \n",
    "\n",
    "            # dropout, żeby przeciwdziałać overfittingowi\n",
    "            nn.Dropout(p=0.05),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc1174",
   "metadata": {},
   "source": [
    "Dropout zapobiega przeuczaniu się sieci konwolucyjnej, lecz przy naszej sieci i małym zbiorze uczącym ma ona znikomy wpływ, lub czasem negatywny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "94feff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net1 = Net1(num_classes=7)\n",
    "net2 = Net2(num_classes=7)\n",
    "net3 = Net3(num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8bb3f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.Adam(net1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(net2.parameters(), lr=0.001)\n",
    "optimizer3 = optim.Adam(net3.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for images, labels in dataloader_train:\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        outputs1 = net1(images)\n",
    "        outputs2 = net2(images)\n",
    "        outputs3 = net3(images)\n",
    "        loss1 = criterion(outputs1, labels)\n",
    "        loss2 = criterion(outputs2, labels)\n",
    "        loss3 = criterion(outputs3, labels)\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        loss3.backward()\n",
    "        optimizer3.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1fbb560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metric_precision = Precision(\n",
    "    task = \"multiclass\",\n",
    "    num_classes=7,\n",
    "    average=\"macro\"\n",
    "    )\n",
    "\n",
    "metric_recall = Recall(\n",
    "    task = \"multiclass\",\n",
    "    num_classes=7,\n",
    "    average=\"macro\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "82ad1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1 acc: 0.7386831045150757\n",
      "net2 acc: 0.8168724179267883\n",
      "net3 acc: 0.7386831045150757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "def eval_accuracy(net, dataloader, num_classes=7):\n",
    "    net.eval()\n",
    "    acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = net(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc.update(preds, labels)\n",
    "    return acc.compute().item()\n",
    "\n",
    "a1 = eval_accuracy(net1, dataloader_test, 7)\n",
    "a2 = eval_accuracy(net2, dataloader_test, 7)\n",
    "a3 = eval_accuracy(net3, dataloader_test, 7)\n",
    "\n",
    "print(\"net1 acc:\", a1)\n",
    "print(\"net2 acc:\", a2)\n",
    "print(\"net3 acc:\", a3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
